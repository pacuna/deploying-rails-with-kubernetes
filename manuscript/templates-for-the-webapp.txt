### Templates for the Rails application

#### Preparing our application for production

Before we're able to run our application in the production environment we have
make some changes.

First, let's add the correct endpoints for the production Database and Search Engine.

Open the config/database.yml file and add:

    production:
      adapter: postgresql
      encoding: unicode
      database: myapp_production
      username: postgres
      host: postgres.production
      password: secretpassword
      pool: 5

The Kubernetes convention for the DNS of a resource that's running in a namespace
different than the default one, is to use the service name, followed by a dot
and the namespace. Our Database service name is `postgres`, and the namespace
is `production` so we have to use `postgres.production` and that will
return the internal endpoint for the DB.

Now open the `config/initializers/elasticsearch.rb` file and modify the configuration
by changing it to:

    if Rails.env.production?
      host = 'elasticsearch.production'
    else
      host = 'elasticsearch'
    end

    Elasticsearch::Model.client = Elasticsearch::Client.new host: "http://#{host}:9200"

    # Print Curl-formatted traces in development into a file
    #
    if Rails.env.development?
      tracer = ActiveSupport::Logger.new('log/elasticsearch.log')
      tracer.level =  Logger::DEBUG
    end

    Elasticsearch::Model.client.transport.tracer = tracer

Following the same logic, the internal endpoint for Elasticsearch is represented
by `elasticsearch.production`.
In this case we're adding a conditional for selecting the Elasticsearch
host depending on the environment.

We also need to add a `secret_token` for the production environment. You can get one
by running:

    # rake secret

Inside of the myapp container. Paste that result into the `config/secrets.yml` file
in the correspondent section:

    production:
      secret_key_base: verylongsecrettoken

Finally, we need to add an script than compiles the application assets.
The passenger image provides us a very elegant and easy way to accomplish this.

We are going to add a script during the build of the image but that will be
executed before the container starts.

Open the `Dockerfile`, search for this section and add the following:

    ...
    WORKDIR /home/app/webapp

    RUN mkdir -p /etc/my_init.d
    ADD deploy/start.sh /etc/my_init.d/start.sh
    ...

Now we have to create the `start.sh` file:

    touch deploy/start.sh
    chmod +x deploy/start.sh

And add the following to it:

    #!/bin/sh

    if [ "$PASSENGER_APP_ENV" = "production" ]; then
        mkdir -p tmp/cache
        chown -R app:app tmp
        chmod 777 -R tmp/cache
        chown -R app:app public
        touch log/production.log
        chown -R app:app log
        RAILS_ENV=production bundle install &&
        RAILS_ENV=production bundle exec rake db:create &&
        RAILS_ENV=production bundle exec rake db:migrate &&
        RAILS_ENV=production bundle exec rake assets:precompile --trace
    else
        rm -rf public/assets
    fi

This is a very basic bash script that checks for the `PASSENGER_APP_ENV` variable
during the build. In this case we are only executing some commands for the production
environment. We have to make sure that all of the permissions are correctly setted
for the `tmp` and `log` folders, and then we run all the typical commands for a post deployment 
for a Rails application.

Now we should be ready. It's a good idea to test this setup in your local environment.
For that, you just need to replace the credentials for Postgres and Elasticsearch
for the development ones, and change the `PASSENGER_APP_ENV` variable in your
`docker-compose.yml` from `development` to `production`. Then you need to rebuild
you containers by running `docker-compose build` and `docker-compose up -d` as
usual. This way you'll be simulating a deployment and you can test that all is in order. 
After you test everything is OK, remember to replace the variables
for the production ones.

Now let's build our production-ready version of the image and push it to the DockerHub.
This time we'll tag it with version 2:

    docker build -t username/myapp:2 .
    docker push username/myapp:2

#### Replication Controller

If we want to scale our application at some point, we're going to need
a replication controller instead of a regular pod.
This way we can create replicas for the pod managed by this controller if more
more containers are needed. It is also going to make the deployments of new version
a lot more easier by using rolling-updates.

Let's create the template for the replication controller:

    mkdir -p deploy/kube/rcs
    touch deploy/kube/rcs/myapp-production.yaml

And add the following to the template:

    apiVersion: v1
    kind: ReplicationController
    metadata:
      name: myapp-v2
    spec:
      replicas: 1
      selector:
        app: myapp
        deployment: v2
      template:
        metadata:
          name: myapp
          labels:
            app: myapp
            deployment: v2
        spec:
          containers:
          - name: myapp
            image: username/myapp:2
            ports:
              - containerPort: 80
            env:
              - name: PASSENGER_APP_ENV
                value: production

The kind for this template is `ReplicationController`. The most important
metadata attribute is the name. This name will identify this controller and
it will be used for identifying and replacing the pod during new releases.
Then we have the spec section. First we declare the number of replicas, which
corresponds to the number of containers that will be attached to the Load Balancer.
This value can also be modified without having downtime by running a simple command with
the kubectl tool. Inside of the spec we also need a selector so we can later
attach a service to this controller and its pods. We are also adding a 
selector for the deployment version. This variable will help us later for deploying new
versions.
The template contains the information of the pod that this controller will be
managing. We also need some metadata, and then the specs for the container.
Here we're going to use the image we just pushed to DockerHub along with
the port that should be exposed, and the necessary environment variables.

Now let's create a service for routing the requests to the pods managed
by the controller:

    touch deploy/kube/services/myapp-svc.yaml

And add the following:

    apiVersion: v1
    kind: Service
    metadata:
      name: myapp-service
    spec:
      ports:
      - port: 80
        targetPort: 80
      selector:
        app: myapp
      type: LoadBalancer

The most important part of this template is the type. The type in this case
must be `LoadBalancer`. If we don't use that, we are not going to have an external
address with a Load Balancer routing requests to the controller pods. And that is not what we want.
That's the difference between this kind of service and services for other back-end
elements such a databases and search engines. Those services usually don't need
to be externally open, and it doesn't matter is they are not running in the same host
the application is. That's the magic of container clusterization. You can achieve
a transparent private communication between your services but controlling their exposure.

Let's create the service and the replication controller in our cluster:

    $ kubectl create -f deploy/kube/services/myapp-svc.yaml --namespace production

Output:

    service "myapp-service" deleted

    $ kubectl create -f deploy/kube/rcs/myapp-production.yaml --namespace production

Output:

    replicationcontroller "myapp-v2" created


You can describe a replication controller with:

    $ kubectl describe replicationcontrollers --namespace production

Output:


    Name:       myapp-v2
    Namespace:	production
    Image(s):	pacuna/myapp:2
    Selector:	app=myapp,deployment=v2
    Labels:     app=myapp,deployment=v2
    Replicas:	1 current / 1 desired
    Pods Status:1 Running / 0 Waiting / 0 Succeeded / 0 Failed
    No volumes.
    Events:
      FirstSeen	LastSeen	Count	From				SubobjectPath	Type		Reason			Message
      ---------	--------	-----	----				-------------	--------	------			-------
      1m		1m		1	{replication-controller }			Normal		SuccessfulCreate	Created pod: myapp-v2-i8dll

Which means the pod for our app was successfully created and in my case has a name of `myapp-v2-i8dll`.

Now wait for a few minutes, and run this command in order to see the endpoint for our
Load Balancer:

    $ kubectl describe service myapp --namespace production

Output:

    Name:		myapp-service
    Namespace:		production
    Labels:		<none>
    Selector:		app=myapp
    Type:		LoadBalancer
    IP:			10.0.160.160
    LoadBalancer Ingress:   adff1155b171211e6a279065abcbcf5b-1127508601.us-west-2.elb.amazonaws.com
    Port:		<unset>	80/TCP
    NodePort:		<unset>	30345/TCP
    Endpoints:		10.244.2.4:80
    Session Affinity:	None
    Events:
      FirstSeen	LastSeen	Count	From			SubobjectPath	Type		Reason			Message
      ---------	--------	-----	----			-------------	--------	------			-------
      3m		3m		1	{service-controller }			Normal		CreatingLoadBalancer	Creating load balancer
      3m		3m		1	{service-controller }			Normal		CreatedLoadBalancer	Created load balancer

As you can see in my case, the endpoint is `adff1155b171211e6a279065abcbcf5b-1127508601.us-west-2.elb.amazonaws.com`.
This is the Amazon ELB that Kubernetes created for routing request to the cluster, specifically
to the pods managed by the myapp controller.

So now you can navigate to the address you got and see your application in action. You should be
able to see the `/articles` path and play with the scaffold with no problems.

If the address is not available, wait for a couple of minutes. Sometimes
the Load Balancer can take some time to create, but this is going to be only
the first time, since this endpoint will not change anymore. All the updates
added in future deployments will be reflected in the replication controller and 
not in the service.
