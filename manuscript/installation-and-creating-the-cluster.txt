## Kubernetes

### Basic Concepts

If you've used a container orchestration before you probably know that usually
they are very concept-oriented. All of them have different structures and ways
of managing the cluster, and of course Kubernetes has its own.

In this section I'll talk briefly about the concepts you should know when working
with Kubernetes. All of these elements can be defined inside of yaml templates. This
will provide us a nice way for versioning our infrastructure and for a fast
creation and deletion of resources.

#### Pods

A pod is the most basic unit of deployment in Kubernetes. If you are going to
deploy a container, you have to use a Pod that runs this container. You can
run more than one container in a pod if you want. I haven't seen too many 
scenarios in which you would want to do this, but there are some. 
Normally you would deploy one container inside of a Pod and manage the replicas
by using the next unity: the replication controller.

#### Replication Controller

The replication controller will help up to manage our Pods. If you want to run
several replicas of a Pod, you want to use a replication controller. This controller
will make sure that all the desired number of pods are always running.
You can define the specification of the Pod for a controller in the same template.

#### Services

The service is the element that will allow us to have static endpoints for our
pods. Every time you create new releases all of the old pods will be deleted
and new ones will be placed and scheduled in the cluster. Because of that, you cannot
use pods addresses for connecting your applications.
The other limitation arises when you have a replication controller managing
more than one pod. How are going to balance the requests to those pods? The answer
for both of these problems are services. A service can be attached to a pod
or a controller and have a static endpoint that allows communication with other services.
It also can act as a Load Balancer if you declare that's what you want in the service
template. There are several types of services in Kubernetes but in this books we'll be
using the basic ones, internals and external Load Balancers.

#### Namespaces

Namespaces are defined as virtual clusters inside of the real cluster. In a few
words, they allow you to have resource isolation for different environments that
you may want to have in the cluster. We're going to use namespaces for controlling
the resources for our production and staging environments.

Like I said before, all of this elements can be defined inside of yaml templates
with certain identifiers that will allow Kubernetes to connect the pieces between
pods, controllers and services.

### Installation and creating the cluster

For installing Kubernetes and creating the cluster, we'll be following
the instructions from the [official documentation](http://kubernetes.io/docs/getting-started-guides/aws/).
By now you should have the AWS CLI installed and your AWS account properly
configured.
Let's add some environmental variables to our rc profile (.zshrc or .bashrc depending
on what shell you're using):

    export KUBERNETES_PROVIDER=aws
    export NUM_NODES=2
    export MASTER_SIZE=t2.small
    export NODE_SIZE=t2.small
    export INSTANCE_PREFIX=k8s

Here we are setting the cluster configuration, which for now will have
2 machines of site t2.small using the default kubernetes region (us-west-2a).
Now run a `source` command against the file so the variables are exported 
for the current sessions.

    $ source FILE

We can run one command that will install Kubernetes and will create the cluster for us.

    cd ~
    export KUBERNETES_PROVIDER=aws; curl -sS https://get.k8s.io | bash

After running this command, you'll see a lot of output. The instances will be launched, 
the security groups will be created, etc. You'll have to wait a couple of minutes
until the cluster es created correctly.
Once it finishes, the last part of the log should say something like this:

    Kubernetes cluster is running.  The master is running at:

      https://KUBE_IP_ADDRESS

    The user name and password to use is located in /Users/pacuna/.kube/config.

    ... calling validate-cluster
    Waiting for 2 ready nodes. 0 ready nodes, 0 registered. Retrying.
    Waiting for 2 ready nodes. 0 ready nodes, 1 registered. Retrying.
    Waiting for 2 ready nodes. 1 ready nodes, 2 registered. Retrying.
    Found 2 node(s).
    NAME                                        STATUS    AGE
    ip-172-20-0-20.us-west-2.compute.internal   Ready     37s
    ip-172-20-0-21.us-west-2.compute.internal   Ready     52s
    Flag --api-version has been deprecated, flag is no longer respected and will be deleted in the next release
    Validate output:
    NAME                 STATUS    MESSAGE              ERROR
    controller-manager   Healthy   ok
    scheduler            Healthy   ok
    etcd-0               Healthy   {"health": "true"}
    etcd-1               Healthy   {"health": "true"}
    Cluster validation succeeded
    Done, listing cluster services:

    Kubernetes master is running at https://KUBE_IP_ADDRESS
    Elasticsearch is running at https://KUBE_IP_ADDRESS/api/v1/proxy/namespaces/kube-system/services/elasticsearch-logging
    Heapster is running at https://KUBE_IP_ADDRESS/api/v1/proxy/namespaces/kube-system/services/heapster
    Kibana is running at https://KUBE_IP_ADDRESS/api/v1/proxy/namespaces/kube-system/services/kibana-logging
    KubeDNS is running at https://KUBE_IP_ADDRESS/api/v1/proxy/namespaces/kube-system/services/kube-dns
    kubernetes-dashboard is running at https://KUBE_IP_ADDRESS/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
    Grafana is running at https://KUBE_IP_ADDRESS/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
    InfluxDB is running at https://KUBE_IP_ADDRESS/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb


You can see Kubernetes is telling you the IP address of your cluster, along
with all the services that come by default. Some of these services
are very useful for inspecting and monitoring your cluster.
Before going to this endpoints, let's see what the script has created for us
in our aws account. We are going to use the CLI for interacting with our
resources.
First let's list the EC2 Instances:

    aws ec2 describe-instances --query 'Reservations[*].Instances[*].[State.Name, InstanceId]' --output text --region us-west-2

Output:

    running	i-ba8a9962
    running	i-bd8a9965
    running	i-aa8a9972

So you can see that we have 3 instances. Two of these are nodes and one corresponds
to a master node. 
To add more nodes to our cluster will be very easy task, since the script
has also created an auto-scaling group for us:

    aws autoscaling describe-auto-scaling-groups --output text --region us-west-2 --query 'AutoScalingGroups[*].AutoScalingGroupName'

Output:

    kubernetes-minion-group-us-west-2a

With this autoscaling group, we can add nodes to our cluster with no further
configuration. As a matter of fact, let's add 2 more nodes to our cluster, since
we will need a couple more resources for running our application with all its
dependencies. Using the name from the previous output run the following command:

    aws autoscaling update-auto-scaling-group --auto-scaling-group-name kubernetes-minion-group-us-west-2a --min-size 4 --max-size 4 --desired-capacity 4 --region us-west-2

This command will update the number of instance of the group and our cluster.
You can also do this using the console panel from AWS (although using commmands
is way more reusable).

After a couple of minutes, if you list your instances again:

    aws ec2 describe-instances --query 'Reservations[*].Instances[*].[State.Name, InstanceId]' --output text --region us-west-2

Output:

    running	i-cab1a212
    running	i-cbb1a213
    running	i-ba8a9962
    running	i-bd8a9965
    running	i-aa8a9972

You'll see that now we have five instances running. Four nodes and one master node.

### Inspecting the cluster configuration and endpoints

Kubernetes comes with a very handy and useful command line tool called `kubectl`.
Let's add the binary to our path so we can use it with no problems.
Assuming you installed Kubernetes in your home path (~), add the following to
your .bashrc or .zshrc file:

    export PATH=~/kubernetes/platforms/darwin/amd64:$PATH

And remember to source that file or restart your terminal. Let's see if the
tool is working:

    kubectl get nodes

Output:

    NAME                                        STATUS    AGE
    ip-172-20-0-20.us-west-2.compute.internal   Ready     32m
    ip-172-20-0-21.us-west-2.compute.internal   Ready     32m
    ip-172-20-0-57.us-west-2.compute.internal   Ready     38s
    ip-172-20-0-58.us-west-2.compute.internal   Ready     43s

The `get nodes` commands is giving us the names of all of the nodes of the cluster.
The configuration for this tool is being read from the ~/.kube folder that
Kubernetes created during the launching process. All of our cluster configuration, 
including addresses, users and passwords are there, so you have to be careful
is you're playing with those files.
If you want to display this configuration in a more friendly way, you can run:

    kubectl config view

Output:

    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority-data: REDACTED
        server: https://KUBE_IP_ADDRESS
      name: aws_kubernetes
    contexts:
    - context:
        cluster: aws_kubernetes
        user: aws_kubernetes
      name: aws_kubernetes
    current-context: aws_kubernetes
    kind: Config
    preferences: {}
    users:
    - name: aws_kubernetes
      user:
        client-certificate-data: REDACTED
        client-key-data: REDACTED
        token: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
    - name: aws_kubernetes-basic-auth
      user:
        password: xxxxxxxxxxxxxxxx
        username: xxxxx

There you have the IP address of our cluster and the credentials for login into it.

If you open your browser and visit your cluster's IP address, you'll need to
fill out the form using these credentials. Once your on the root path, you'll
see all of the available endpoints. For now let's check the main dashboard.
Go to `https://IP_ADDRESS/ui`:

You'll see the beautiful Kubernetes dashboard. If you click on `Show them`, you'll
see all of the current services running on our cluster. All of these application, such
as Elasticsearch, Kibana, Graphana, the DNS server, InfluxDB, etc, are running in containers
that belong to a system namespace of Kubernetes. What we want, is to run
our own containers in the cluster.

You can see that Kubernetes has taken all of the complexity that comes with creating a cluster, security
and configuration, and lets you focus only on deploying your applications.
