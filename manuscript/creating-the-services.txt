### Creating pods and services

The pods and services for the Database and the Search Engine will be moderately simple.
The only complexity introduced by this kind of containers is how is data going to
be persisted. Another good thing about Kubernetes is that it has a solid 
integration with [Amazon EBS](https://aws.amazon.com/ebs/)
which is the persistence storage that AWS offers. These blocks will hold our
data while the containers can be floating around through the cluster.

Let's do something before so we can keep our code organized. We are going to
create a folder in the root of the project that will contain all of our Kubernetes related stuff.
Let's call it `kube`:

    $ mkdir -p deploy/kube

Second, let's create a template for the production namespace in which we'll
deploy our services. This way we can keep separated the different environments
(production, staging in our case) that we'll be using in our cluster.

    $ mkdir -p deploy/kube/namespaces
    $ touch deploy/kube/namespaces/production.yaml

And add the following to that template:

    apiVersion: v1
    kind: Namespace
    metadata:
      name: production

Namespaces are a good way of having isolation inside of the cluster. A common scenario 
is to have different namespaces for different environment, for example production
and staging.

Now we can use the kubectl tool for creating the namespace in the cluster:

    $ kubectl create -f deploy/kube/namespaces/production.yaml

Output:

    namespace "production" created
    
#### Data Persistence

Before start writing the templates for the pods, let's create the two AWS EBS
that will hold the data for Postgres and Elasticsearch. We can create both
of these volumes by using the AWS CLI that was previously configured.

Run the following command to create the Postgres Volume:

    aws ec2 create-volume --availability-zone us-west-2a --size 10 --volume-type gp2 --region us-west-2

Output:

    {
        "AvailabilityZone": "us-west-2a",
        "Encrypted": false,
        "VolumeType": "gp2",
        "VolumeId": "SOME_VOLUME_ID",
        "State": "creating",
        "Iops": 30,
        "SnapshotId": "",
        "CreateTime": "2016-05-10T01:16:45.916Z",
        "Size": 10
    }

The value we'll need is the `VolumeId`. So save that for later.

Let's created another one for Elasticsearch by using the same command:

    aws ec2 create-volume --availability-zone us-west-2a --size 10 --volume-type gp2 --region us-west-2

Output:

    {
        "AvailabilityZone": "us-west-2a",
        "Encrypted": false,
        "VolumeType": "gp2",
        "VolumeId": "SOME_VOLUME_ID",
        "State": "creating",
        "Iops": 30,
        "SnapshotId": "",
        "CreateTime": "2016-05-10T01:18:33.097Z",
        "Size": 10
    }

You should also Write down this `VolumeId` for later usage. We are going to use both 
of this Volumes IDs for the pod templates.

#### Pods

Now that we have our two AWS EBSs for persisting the data, let's create the templates
for the database and for the search engine.

Let's start with the database. First create the template:

    mkdir -p deploy/kube/pods
    touch deploy/kube/pods/postgres.yaml

And add the following to that template. Replace the `volumeId` in the 
volumes section with the first one you got earlier:

    apiVersion: v1
    kind: Pod
    metadata:
      name: postgres
      labels: 
        name: postgres
    spec: 
      containers: 
        - resources:
          image: postgres:9.4
          name: postgres
          env:
            - name: POSTGRES_PASSWORD
              value: secretpassword
            - name: PGDATA
              value: /var/lib/postgresql/data/pgdata
          ports: 
            - containerPort: 5432
              name: postgres
          volumeMounts:
            - name: postgres-persistent-storage
              mountPath: /var/lib/postgresql/data
      volumes:
        - name: postgres-persistent-storage
          awsElasticBlockStore:
            volumeID: VOLUME_ID
            fsType: ext4

This template may look complicated but it's actually pretty simple.
First, we're adding some metadata so later we can attach a service to this pod.
Then we have the spec section, which contains all the container information for
deploying the pod. In this case, as we are mounting a volume for our data, we have
to add a couple of extra configuration for the `PGDATA` value. You can find
more information about that variable in the official DockerHub page for this
image.
You can see that inside the container specification, we have a section for
the volume mounts. There we can assign a name that later must mach a value
declared in the volumes section. In that last section we need the `volumeID`
we picked up before, and the AWS specification.

Now let's do the same for Elasticsearch:

    $ touch deploy/kube/pods/elasticsearch.yaml

And add the following to the template also replacing the `VolumeId` with the second
one created:

    apiVersion: v1
    kind: Pod
    metadata:
      name: elasticsearch
      labels: 
        name: elasticsearch
    spec: 
      containers: 
        - resources:
          image: elasticsearch:2
          name: elasticsearch
          ports: 
            - containerPort: 9200
              name: http
              protocol: TCP
            - containerPort: 9300
              name: transport
              protocol: TCP
          volumeMounts:
            - name: elasticsearch-persistent-storage
              mountPath: /usr/share/elasticsearch/data
      volumes:
        - name: elasticsearch-persistent-storage
          awsElasticBlockStore:
            volumeID: VOLUME_ID
            fsType: ext4

Pretty similar to the database one.

And now we can launch the pods in our production namespace:

    $ kubectl create -f deploy/kube/pods/postgres.yaml --namespace production

Output:

    pod "postgres" created

    $ kubectl create -f deploy/kube/pods/elasticsearch.yaml --namespace production

Output:

    pod "elasticsearch" created


We can list the pods with this command:

    kubectl get pods --namespace production

Output:

    NAME            READY     STATUS    RESTARTS   AGE
    elasticsearch   1/1       Running   0          12s
    postgres        1/1       Running   0          24s

As you can see, both of the pods were successfully started.

#### Services

The reason we need a service over the pod, is that the pod address will be
changing with every release. Containers will be destroyed and started, so
we need a static DNS identifiers for the endpoints.
What we need in this case, is a service that can be attached to a pod
and route requests to it.

Let's create the folder and the services templates:

    mkdir -p deploy/kube/services
    touch deploy/kube/services/postgres-svc.yaml
    touch deploy/kube/services/elasticsearch-svc.yaml

Add the following to the `postgres-svc.yaml` file:

    apiVersion: v1
    kind: Service
    metadata: 
      labels: 
        name: postgres
      name: postgres
    spec: 
      ports:
        - port: 5432
      selector: 
        name: postgres

And for the `elasticsearch-svc.yaml` file:

    apiVersion: v1
    kind: Service
    metadata: 
      labels: 
        name: elasticsearch
      name: elasticsearch
    spec: 
      ports:
        - name: http
          port: 9200
          protocol: TCP
        - name: transport
          port: 9300
          protocol: TCP
      selector: 
        name: elasticsearch

These template are also very self-explanatory. We need a metadata, but most
important, the specfication of the ports that this service should respond, and
the selector of the pod the service should attach to. In this case, the selector
corresponds to the pod name declared in its metadata.
This services don't need to be externally exposed, so we don't need to add type
to the template.

Now let's launch these services:

    kubectl create -f deploy/kube/services/postgres-svc.yaml --namespace production

Output:

    service "postgres" created

And:

    kubectl create -f deploy/kube/services/elasticsearch-svc.yaml --namespace production

Output:

    service "elasticsearch" created

Let's see the services in our cluster:

    $ kubectl get services --namespace production

Output:

    NAME            CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
    elasticsearch   10.0.204.179   <none>        9200/TCP,9300/TCP   31s
    postgres        10.0.139.208   <none>        5432/TCP            1m

As you can see, we only have internal IPs for this services. We don't need
external addresses since these services should only establish
connections with our web app. Our application is going to able to reach the endpoints
of these services by using the service name declared in the templates, which are
`postgres` and `elasticsearch` but following the Kubernetes convention of putting a dot 
and the namespace after. So the real DNS aliases will be `postgres.production` and
`elasticsearch.production`.
